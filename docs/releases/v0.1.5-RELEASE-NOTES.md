# Release v0.1.5 - Namespace Fixes, Helm Updates, and Storage Infrastructure

**Release Date**: February 8, 2026  
**Branch**: `v0.1.5`  
**Status**: In Progress

## Overview

This release focuses on fixing systemic namespace issues across all CDK8s applications, updating Helm charts to latest stable versions, and establishing proper storage infrastructure with Longhorn on Talos Linux.

## Table of Contents

- [Critical Issues Resolved](#critical-issues-resolved)
- [Applications Modified](#applications-modified)
- [Infrastructure Changes](#infrastructure-changes)
- [Deployment Timeline](#deployment-timeline)
- [Breaking Changes](#breaking-changes)
- [Migration Guide](#migration-guide)
- [Verification Steps](#verification-steps)

---

## Critical Issues Resolved

### 1. Systemic Namespace Issues

**Problem**: All CDK8s applications were deploying resources to incorrect namespaces due to using CDK8s-generated Helm imports instead of `cdk8s.NewHelm`.

**Impact**:
- Secrets deployed to `default` namespace instead of application namespaces
- ArgoCD showing resources as OutOfSync
- Security concerns with cross-namespace secret access

**Solution**: Converted all 14 applications to use `cdk8s.NewHelm` with explicit `Namespace` parameter.

**Applications Fixed**:
1. Longhorn (longhorn-system)
2. Harbor (harbor)
3. Grafana (grafana)
4. VictoriaMetrics (victoria-metrics)
5. VictoriaLogs (victoria-logs)
6. AlertManager (alertmanager)
7. Cert-Manager (cert-manager)
8. Kyverno (kyverno)
9. Trivy (trivy-system)
10. Nvidia GPU Operator (gpu-operator)
11. Ollama (ollama)
12. Fleet (fleet-system)
13. N8n (n8n)
14. Headlamp (headlamp)

### 2. Longhorn Storage Infrastructure

**Problem**: Longhorn deployment failing on Talos Linux due to:
- Pre-upgrade hook blocking initial installation
- Incorrect node selector (workers don't have `node-role.kubernetes.io/worker` label)
- Missing PSA privileged labels on namespace

**Solution**:
- Disabled pre-upgrade checker: `preUpgradeChecker.jobEnabled: false`
- Replaced node selectors with control-plane tolerations
- Added PSA labels to namespace for privileged workloads

**Result**: Longhorn manager DaemonSet can now schedule on all 4 worker nodes.

### 3. Infisical Deployment

**Problem**: Infisical pods in CrashLoopBackOff due to:
- PostgreSQL and Redis deploying to `default` namespace
- No StorageClass specified for PVCs
- Incorrect Helm chart version

**Solution**:
- Converted to `cdk8s.NewHelm` with explicit namespace
- Updated chart version from `0.7.2` to `1.7.2`
- Added `storageClass: longhorn` to PostgreSQL and Redis persistence

---

## Applications Modified

### Helm Chart Version Updates

| Application | Old Version | New Version | Notes |
|-------------|-------------|-------------|-------|
| Longhorn | 1.7.3 | 1.11.0 | Major version upgrade |
| Harbor | 1.16.1 | 1.18.2 | Patch update |
| Grafana | 8.8.2 | 10.5.15 | Major version upgrade |
| VictoriaMetrics | - | - | No change |
| VictoriaLogs | - | - | No change |
| AlertManager (kube-prometheus-stack) | 67.7.0 | 81.5.0 | Major version upgrade |
| Cert-Manager | v1.16.2 | v1.19.3 | Minor version upgrade |
| Kyverno | 3.3.6 | 3.7.0 | Minor version upgrade |
| Trivy Operator | 0.26.0 | 0.31.0 | Minor version upgrade |
| Nvidia GPU Operator | v24.9.0 | v25.10.1 | Minor version upgrade |
| Ollama | 1.40.0 | 1.41.0 | Patch update |
| Fleet | 0.11.0 | 0.14.2 | Minor version upgrade |
| Infisical | 0.7.2 | 1.7.2 | Chart versioning changed |

### File Changes

#### Storage
- `platform/cdk8s/cots/storage/longhorn.go`
  - Added namespace creation with PSA labels
  - Disabled pre-upgrade checker
  - Replaced node selectors with tolerations
  - Configured for Talos Linux

#### Security & Compliance
- `platform/cdk8s/cots/seccomp/infisical.go`
  - Converted to `cdk8s.NewHelm`
  - Updated chart version to 1.7.2
  - Added `storageClass: longhorn` to PostgreSQL and Redis
  - Fixed namespace parameter

- `platform/cdk8s/cots/seccomp/cert_manager.go`
  - Updated to v1.19.3
  - Added namespace parameter

- `platform/cdk8s/cots/seccomp/keyverno.go`
  - Updated to 3.7.0
  - Added namespace parameter

- `platform/cdk8s/cots/seccomp/trivy.go`
  - Updated to 0.31.0
  - Added namespace parameter

#### Monitoring
- `platform/cdk8s/cots/monitoring/grafana.go`
  - Updated to 10.5.15 (major upgrade)
  - Added namespace parameter

- `platform/cdk8s/cots/monitoring/alert_manager.go`
  - Updated kube-prometheus-stack to 81.5.0
  - Added namespace parameter

#### Registry
- `platform/cdk8s/cots/registry/harbor.go`
  - Updated to 1.18.2
  - Added namespace parameter

#### AI/ML
- `platform/cdk8s/cots/ai/ollama.go`
  - Updated to 1.41.0
  - Added namespace parameter

- `platform/cdk8s/cots/ai/nvidia_gpu_operator.go`
  - Updated to v25.10.1
  - Added namespace parameter

#### Management
- `platform/cdk8s/cots/management/fleet_device_manager.go`
  - Updated to 0.14.2
  - Added namespace parameter

- `platform/cdk8s/cots/management/headlamp.go`
  - Added namespace parameter
  - Fixed OIDC secret namespace

#### Automation
- `platform/cdk8s/cots/automation/n8n.go`
  - Added namespace parameter to ChartProps and HelmProps

---

## Infrastructure Changes

### Talos Linux Configuration

**Longhorn Requirements on Talos**:
- ✅ Kubernetes v1.35.0 (required ≥1.25)
- ✅ Containerd v2.0.2 (required ≥1.3.7)
- ✅ 4 worker nodes (required ≥3)
- ✅ iscsid daemon (included in Talos)
- ✅ NFS client (integrated in kubelet)

**Configuration Applied**:
```go
// PSA labels for privileged workloads
Labels: {
  "pod-security.kubernetes.io/enforce": "privileged",
  "pod-security.kubernetes.io/audit": "privileged",
  "pod-security.kubernetes.io/warn": "privileged",
}

// Disable pre-upgrade hook for GitOps
"preUpgradeChecker": {
  "jobEnabled": false,
}

// Tolerations instead of node selectors
"longhornManager/Driver/UI": {
  "tolerations": [{
    "key": "node-role.kubernetes.io/control-plane",
    "operator": "Exists",
    "effect": "NoSchedule",
  }],
}
```

### Storage Classes

**New StorageClass**: `longhorn` (default)
- Provisioner: `driver.longhorn.io`
- Replica Count: 3
- Data Path: `/var/lib/longhorn/`

---

## Deployment Timeline

### Phase 1: Namespace Fixes (Completed)
- [x] Identified systemic namespace issue
- [x] Fixed 11 core applications
- [x] Fixed N8n and Headlamp
- [x] Fixed Infisical

### Phase 2: Helm Updates (Completed)
- [x] Audited all Helm chart versions
- [x] Updated 9 charts to latest stable
- [x] Tested major version upgrades

### Phase 3: Longhorn Deployment (In Progress)
- [x] Added namespace with PSA labels
- [x] Disabled pre-upgrade checker
- [x] Fixed node selector issue
- [ ] Verify DaemonSet deployment
- [ ] Verify StorageClass creation

### Phase 4: Infisical Deployment (Pending)
- [x] Fixed namespace configuration
- [x] Added StorageClass to PVCs
- [ ] Verify PVC binding
- [ ] Verify pod startup
- [ ] Access UI

---

## Breaking Changes

### 1. Namespace Changes

**Impact**: Resources will be recreated in correct namespaces.

**Action Required**:
- ArgoCD will show resources as `requiresPruning: true` in old namespaces
- Manual cleanup may be needed for orphaned resources

**Cleanup Commands**:
```bash
# List resources requiring pruning
kubectl get all -n default -l app.kubernetes.io/instance=<app-name>

# Delete orphaned resources
kubectl delete <resource> -n default <name> --cascade=orphan
```

### 2. Helm Chart Major Upgrades

**Grafana**: 8.x → 10.x
- Review breaking changes in Grafana 9.x and 10.x releases
- Check dashboard compatibility

**AlertManager (kube-prometheus-stack)**: 67.x → 81.x
- Review Prometheus Operator changes
- Verify CRD updates

**Longhorn**: 1.7.x → 1.11.x
- Review storage compatibility
- Check volume migration requirements

---

## Migration Guide

### Pre-Migration Checklist

1. **Backup Current State**
   ```bash
   # Backup all namespaces
   kubectl get all --all-namespaces -o yaml > backup-pre-v0.1.5.yaml
   
   # Backup secrets
   kubectl get secrets --all-namespaces -o yaml > secrets-backup.yaml
   ```

2. **Verify ArgoCD Status**
   ```bash
   kubectl get applications -n argocd
   ```

3. **Check Current PVCs**
   ```bash
   kubectl get pvc --all-namespaces
   ```

### Migration Steps

1. **Sync Longhorn First**
   - Wait for CI pipeline to complete
   - Monitor ArgoCD sync for Longhorn
   - Verify StorageClass creation

2. **Sync Infisical**
   - Wait for Longhorn StorageClass
   - Monitor PVC binding
   - Verify pod startup

3. **Sync Remaining Applications**
   - Monitor namespace transitions
   - Verify secrets are in correct namespaces

4. **Cleanup Old Resources**
   - Prune resources in `default` namespace
   - Verify no orphaned resources

### Post-Migration Verification

```bash
# Verify Longhorn
kubectl get pods -n longhorn-system
kubectl get storageclass

# Verify Infisical
kubectl get pods -n infisical
kubectl get pvc -n infisical

# Verify all applications
kubectl get applications -n argocd
```

---

## Verification Steps

### 1. Longhorn Verification

```bash
# Check DaemonSet
kubectl get daemonset -n longhorn-system
# Expected: longhorn-manager 4/4

# Check Deployments
kubectl get deployment -n longhorn-system
# Expected: longhorn-driver-deployer 1/1, longhorn-ui 2/2

# Check StorageClass
kubectl get storageclass
# Expected: longhorn (default)
```

### 2. Infisical Verification

```bash
# Check PVCs
kubectl get pvc -n infisical
# Expected: data-postgresql-0 Bound, redis-data-redis-master-0 Bound

# Check Pods
kubectl get pods -n infisical
# Expected: postgresql-0 Running, redis-master-0 Running, infisical pods Running

# Access UI
curl -k https://infisical.madhan.app
```

### 3. Namespace Verification

```bash
# Check all applications have resources in correct namespaces
for app in longhorn harbor grafana victoria-metrics infisical; do
  echo "=== $app ==="
  kubectl get all -n $app 2>/dev/null || echo "No resources"
done
```

---

## Commits

### Infisical Fixes
- `da258c6` - Converted Infisical to cdk8s.NewHelm for namespace fix
- `93ca416` - Updated Infisical chart version to 1.7.2
- `536efd1` - Added Longhorn StorageClass to PostgreSQL and Redis

### Longhorn Fixes
- `ea6558e` - Added Talos configuration (superseded)
- `a515175` - Added PSA labels and disabled pre-upgrade checker
- `609bcd5` - **Fixed node selector issue** (replaced with tolerations)

### Helm Updates
- Multiple commits updating 9 Helm charts to latest stable versions

### Namespace Fixes
- Multiple commits fixing namespace parameters for 14 applications

---

## Known Issues

### 1. Talos Prerequisites (To Verify)

The following Talos prerequisites should be verified:
- System extensions: `iscsi-tools`, `util-linux-tools`
- Kernel modules: `nbd`, `iscsi_tcp`, `iscsi_generic`, `configfs`

**Verification Commands**:
```bash
# Check system extensions
talosctl get extensions

# Check kernel modules
talosctl -n <worker-ip> read /proc/modules | grep -E "nbd|iscsi|configfs"

# Check iscsid status
talosctl -n <worker-ip> service iscsid status
```

### 2. Old Resources in Default Namespace

Some resources may remain in `default` namespace marked as `requiresPruning: true` in ArgoCD.

**Resolution**: Manual cleanup after verifying new resources are healthy.

---

## Future Work

### v0.1.6 Planning

1. **Enable Longhorn Pre-Upgrade Checker**
   - Re-enable after initial deployment succeeds
   - Set `preUpgradeChecker.jobEnabled: true`

2. **Infisical Setup**
   - Create admin account
   - Generate service tokens
   - Configure projects and environments

3. **Secret Migration**
   - Migrate secrets from Sealed Secrets to Infisical
   - Update applications to use InfisicalSecret CRDs

4. **Monitoring Enhancements**
   - Configure Grafana dashboards for Longhorn
   - Set up alerts for storage capacity

---

## References

- [Longhorn Documentation](https://longhorn.io/docs/)
- [Longhorn on Talos](https://www.talos.dev/v1.9/kubernetes-guides/configuration/storage/#longhorn)
- [Infisical Helm Chart](https://artifacthub.io/packages/helm/infisical/infisical-standalone)
- [CDK8s Documentation](https://cdk8s.io/)

---

## Contributors

- AI Assistant (Antigravity)
- @madhank93

---

**Last Updated**: February 23, 2026

---

## Changes Log

### 2026-02-23 21:00 UTC — Sealed Secrets Controller: ImagePullBackOff + Key Rotation

**Problem**:
- Infisical (and all other apps using SealedSecrets) were `Degraded` in ArgoCD.
- Main infisical pod failing with `Error: secret "infisical-secrets" not found` / `CreateContainerConfigError`.
- Sealed-secrets controller was in `ImagePullBackOff` for 14h with error:
  `Failed to pull image "docker.io/ghcr.io/bitnami-labs/sealed-secrets-controller:0.35.0": pull access denied`
- Root cause: Helm's default `image.registry: docker.io` was prepended to the custom `image.repository: ghcr.io/...`, producing the malformed double-registry URI `docker.io/ghcr.io/...`.
- Because the controller never ran, a **new** keypair was generated on first successful start, making all existing SealedSecrets unreadable (`no key could decrypt secret`).

**Solution**:
1. Live-patched the controller deployment image to `ghcr.io/bitnami-labs/sealed-secrets-controller:0.35.0`.
2. Fetched the new controller public cert: `kubeseal --fetch-cert ... > platform/cdk8s/sealed-secrets-cert.pem`.
3. Fixed `sealedsecrets.go` to set `"registry": ""` in Helm values, preventing future recurrence.
4. Fixed the rendered manifest `app/sealed-secrets/Deployment...yaml` (overwritten on next pipeline run anyway).
5. Pushing this commit triggers the pipeline to re-seal all secrets with the new cert.

**Debugging Steps**:
1. `kubectl get pods -n infisical` → `CreateContainerConfigError` on infisical pod.
2. `kubectl describe pod` → `Error: secret "infisical-secrets" not found`.
3. `kubectl logs -n kube-system sealed-secrets-controller-...` → controller in `ImagePullBackOff`.
4. `kubectl describe pod -n kube-system` → `docker.io/ghcr.io/...` malformed image URI.
5. `kubectl get secrets -n kube-system -l sealedsecrets.bitnami.com/sealed-secrets-key` → only 1 brand-new key (old key lost).
6. Controller logs after fix → `ErrUnsealFailed: no key could decrypt secret` — confirmed key mismatch.

**Files Modified**:
- `platform/cdk8s/cots/seccomp/sealedsecrets.go` — added `"registry": ""`
- `platform/cdk8s/sealed-secrets-cert.pem` — updated with new controller public cert
- `docs/sealed-secrets-key-rotation.md` — **new** runbook for this scenario

---

### 2026-02-23 22:38 UTC — Longhorn: No Disks on Worker Nodes → Infisical PostgreSQL/Redis Stuck

**Problem**:
- After secrets decrypted successfully, infisical still failing with `EPERM 10.100.249.191:5432`.
- `postgresql-0` and `redis-master-0` stuck in `ContainerCreating` for 15h with `AttachVolume failed: volume is not ready for workloads`.
- Longhorn volumes `data-postgresql-0` and `redis-data-redis-master-0` both in **`faulted`** state.
- Longhorn manager logs: `no disks found on node k8s-worker1/2/3/4` for all volumes.
- Root cause: `createDefaultDiskLabeledNodes: true` in Longhorn Helm config means it only provisions the default disk (`/var/lib/longhorn/`) on nodes labelled with `node.longhorn.io/create-default-disk=config`. No worker node had this label → `spec.disks: {}` on all workers → no replicas could be created → all PVCs faulted.

**Solution**:
1. Patched Longhorn setting live: `create-default-disk-labeled-nodes → false`.
2. Manually patched each worker node's `spec.disks` via `kubectl patch` to add `default-disk` at `/var/lib/longhorn/`.
3. Fixed IaC permanently in `longhorn.go`: `"createDefaultDiskLabeledNodes": false`.

**Debugging Steps**:
1. `kubectl describe pod postgresql-0 -n infisical` → `AttachVolume failed: volume not ready`.
2. `kubectl get volumes.longhorn.io -n longhorn-system` → volumes `faulted / detached`.
3. `kubectl get nodes.longhorn.io k8s-worker1 -n longhorn-system -o yaml` → `disks: {}`.
4. `kubectl get settings.longhorn.io` → `create-default-disk-labeled-nodes: true` + no node labels.
5. Live fix: patch setting + patch each node → volumes recovered to `healthy / attached`.

**Files Modified**:
- `platform/cdk8s/cots/storage/longhorn.go` — `createDefaultDiskLabeledNodes: true → false` (defence-in-depth)
- `infra/pulumi/talos.go` — added `machine.kubelet.nodeLabels: node.longhorn.io/create-default-disk: config` to `workerPatch` and `gpuWorkerPatch` (proper long-term fix; new clusters auto-provision Longhorn disks)

---

### 2026-02-24 15:40 UTC — Gateway API Migration: Ingress → HTTPRoute for All COTS Apps

**Problem**:
- "Split-brain" ingress architecture: Infisical used `HTTPRoute` (correctly) while Harbor, Grafana, n8n, Headlamp, and Rancher had `Ingress` objects in their Helm values.
- No cluster-wide Nginx controller installed → apps with `Ingress` objects received no IP and were unreachable.

**Solution**:
- Disabled legacy `Ingress` in all affected apps.
- Added `HTTPRoute` (Gateway API v1) for each app pointing to `homelab-gateway` in `kube-system`.
- TLS handled centrally via the existing wildcard cert `wildcard-madhan-app-tls` (already provisioned by cert-manager in `cert_manager.go` — no per-app certs needed).

**Apps Migrated**:

| App | Ingress Change | HTTPRoute backendRef |
|-----|---------------|----------------------|
| Harbor | `expose.type: ingress` → `clusterIP`, TLS disabled | `harbor-core:80` |
| Grafana | `ingress.enabled: true` → `false` | `grafana:3000` |
| n8n | `ingress.enabled: true` → `false` | `n8n-main:5678` |
| Headlamp | `ingress.enabled: true` → `false` | `headlamp:4466` |
| Rancher | Removed ingress TLS block → `enabled: false` | `rancher:80` |

**Files Modified**:
- `platform/cdk8s/cots/registry/harbor.go`
- `platform/cdk8s/cots/monitoring/grafana.go`
- `platform/cdk8s/cots/automation/n8n.go`
- `platform/cdk8s/cots/management/headlamp.go`
- `platform/cdk8s/cots/management/rancher.go`

**Verification**:
- `go build ./...` → exit 0 (all Go compiles clean)
- Targeted synth of 5 apps → exit 0, 5 `HTTPRoute` YAMLs generated, 0 `Ingress` YAMLs generated
