# NVIDIA GPU on Talos Linux — Complete Setup Guide

This document captures the full journey of getting an NVIDIA GPU working on Talos Linux with the GPU operator and Kubernetes. It is hard-won knowledge built from many debugging cycles.

---

## Table of Contents

- [Hardware](#hardware)
- [Talos Extensions](#talos-extensions)
- [GPU Operator on Talos — Key Behaviors](#gpu-operator-on-talos--key-behaviors)
- [The CDI Problem and Solution](#the-cdi-problem-and-solution)
- [The Validation Bridge DaemonSet](#the-validation-bridge-daemonset)
- [runtimeClassName: nvidia](#runtimeclassname-nvidia)
- [Final Working Configuration](#final-working-configuration)
- [Errors Encountered and Fixes](#errors-encountered-and-fixes)
- [Verification Commands](#verification-commands)

---

## Hardware

| Property | Value |
|----------|-------|
| GPU | RTX 5070 Ti |
| VRAM | 16 GB GDDR7 |
| Driver | 570.211.01 |
| CUDA version | 12.8 |
| Node | k8s-worker4 (`192.168.1.224`) |

---

## Talos Extensions

Talos Linux is an immutable OS that does not permit traditional package installation or kernel module loading. GPU support is provided by two system extensions applied to the worker node via `talos.go`:

### nvidia-open-gpu-kernel-modules-production

- Provides open-source NVIDIA kernel modules compiled for the Talos kernel.
- Provides CUDA userspace libraries at a non-standard path: `/usr/local/glibc/usr/lib/`.
- This path is **critical**: the standard Linux path `/usr/lib/` does not contain NVIDIA libraries on Talos. Any tooling that assumes the standard path will fail.

### nvidia-container-toolkit-production

- Writes `/etc/cri/conf.d/10-nvidia-container-runtime.part` — the containerd configuration that registers `nvidia-container-runtime` as the handler for the `nvidia` RuntimeClass.
- When a pod uses `runtimeClassName: nvidia`, containerd routes that pod through `nvidia-container-runtime`, which injects GPU devices into the container namespace.

When these extensions are installed, GPU operator v25.x detects them via node labels (`extensions.talos.dev/*`) and sets `nvidia.com/gpu.deploy.driver=pre-installed` and `nvidia.com/gpu.deploy.container-toolkit=pre-installed` on the node.

---

## GPU Operator on Talos — Key Behaviors

### Driver DaemonSet DESIRED=0 — This is Correct

When the Talos extension labels are detected, the GPU operator sets:

```
nvidia.com/gpu.deploy.driver=pre-installed
```

This causes the driver DaemonSet to have `DESIRED=0`. **Do not fight this.** It is correct behavior. The Talos extension provides the driver. The GPU operator is correctly recognizing that it does not need to install a driver.

Attempting to override this label manually does not work — the GPU operator controller loop continuously reverts any manual changes to `nvidia.com/gpu.deploy.driver`.

### Toolkit DaemonSet

With `toolkit.enabled: false` in Helm values, the toolkit DaemonSet is also skipped. The Talos `nvidia-container-toolkit-production` extension handles this role.

---

## The CDI Problem and Solution

### What CDI Mode Does

CDI (Container Device Interface) is the default device exposure strategy in GPU operator v25.x. In CDI mode, the device plugin generates a CDI spec file that maps NVIDIA library paths from the host into the container.

### Why CDI Fails on Talos

The CDI spec generated by the device plugin contains entries like:

```json
{
  "type": "bind",
  "options": ["ro"],
  "hostPath": "/usr/lib/libEGL_nvidia.so.x",
  "containerPath": "/usr/lib/libEGL_nvidia.so.x"
}
```

These `hostPath` values reference `/usr/lib/` on the host. On Talos, the NVIDIA libraries are at `/usr/local/glibc/usr/lib/`, not `/usr/lib/`. The bind mounts fail silently, and GPU containers have no libraries to load.

### The Fix: DEVICE_LIST_STRATEGY=envvar

Setting `DEVICE_LIST_STRATEGY=envvar` on the device plugin switches from CDI to environment-variable injection:

```go
"devicePlugin": map[string]any{
    "env": []map[string]any{
        {"name": "DEVICE_LIST_STRATEGY", "value": "envvar"},
    },
},
```

With `envvar` mode:
1. The device plugin injects `NVIDIA_VISIBLE_DEVICES=<uuid>` into the container's environment variables.
2. `nvidia-container-runtime` (from the Talos extension) sees this environment variable.
3. The runtime handles GPU device injection using its own Talos-aware library paths (`/usr/local/glibc/usr/lib/`).

This completely avoids the CDI hostPath problem because the Talos extension's container runtime already knows where to find its own libraries.

**Critical spelling note**: The correct value is `envvar` (singular). `envvars` (plural) is rejected with "invalid strategy: envvars".

---

## The Validation Bridge DaemonSet

### Why It Exists

GPU operator v25.x uses a readiness validation chain before the device plugin starts:

```
.driver-ctr-ready → driver-ready → toolkit-ready → device-plugin starts
```

These files are written to `/run/nvidia/validations/` on the node. In a normal GPU operator deployment, the driver and toolkit DaemonSet containers write these files when they complete successfully.

On Talos, the driver and toolkit DaemonSets have `DESIRED=0` because Talos extensions provide both. The files are never written. The device plugin waits forever in `Init:0/1`.

### The Solution

A `busybox` DaemonSet writes the required marker files on every GPU node at startup:

```go
Command: "/bin/sh -c",
Args: "mkdir -p /run/nvidia/validations && \
       touch /run/nvidia/validations/.driver-ctr-ready \
             /run/nvidia/validations/driver-ready \
             /run/nvidia/validations/toolkit-ready && \
       while true; do sleep 3600; done",
```

This DaemonSet mounts `/run/nvidia/validations` as a `hostPath` so the files persist on the node's filesystem where the device plugin's init containers look for them.

After the bridge DaemonSet runs, the device plugin's init containers succeed and the device plugin starts normally.

---

## runtimeClassName: nvidia

Every workload that needs GPU access must set:

```yaml
spec:
  runtimeClassName: nvidia
```

Without `runtimeClassName: nvidia`, the pod runs through the default containerd runtime, which has no GPU device injection. The GPU resource may be allocated (Kubernetes tracks it as a device), but the container has no access to the actual GPU hardware.

The `nvidia` RuntimeClass is deployed automatically by the GPU operator. It references the `nvidia-container-runtime` handler, which was registered by the Talos `nvidia-container-toolkit-production` extension via `/etc/cri/conf.d/10-nvidia-container-runtime.part`.

---

## Final Working Configuration

### GPU Operator Helm Values (nvidia_gpu_operator.go)

```go
values := map[string]any{
    // driver.enabled=true: GPU operator manages lifecycle but Talos extension provides the actual driver.
    // GPU operator detects extension labels and sets DESIRED=0 on driver DaemonSet automatically.
    "driver": map[string]any{"enabled": true, "nodeSelector": nodeSelector},

    // toolkit.enabled=false: Talos nvidia-container-toolkit-production extension handles this.
    "toolkit": map[string]any{"enabled": false, "nodeSelector": nodeSelector},

    "devicePlugin": map[string]any{
        "nodeSelector": nodeSelector,
        "env": []map[string]any{
            // envvar (singular): avoids CDI hostPath issues on Talos.
            {"name": "DEVICE_LIST_STRATEGY", "value": "envvar"},
        },
    },
    // ... dcgmExporter, gfd also get nodeSelector
}
```

### Workload Configuration (ollama.go, comfyui.go)

```yaml
spec:
  runtimeClassName: nvidia
  nodeSelector:
    nvidia.com/gpu.present: "true"
  containers:
    - name: app
      env:
        - name: NVIDIA_VISIBLE_DEVICES
          value: all
      resources:
        limits:
          nvidia.com/gpu: 1
```

---

## Errors Encountered and Fixes

### 1. Device Plugin CrashLoopBackOff: libcuda.so Not Found

**Symptom**: `nvidia-device-plugin-daemonset` pods in `CrashLoopBackOff`, logs show `libcuda.so: cannot open shared object file`.

**Root cause**: Device plugin tried to load `/usr/lib/libcuda.so` which does not exist on Talos (libraries are at `/usr/local/glibc/usr/lib/`).

**Attempted fix**: Setting `CONTAINER_DRIVER_ROOT=/host/usr/local/glibc` and mounting a hostPath. This approach was superseded.

**Final fix**: Switch to `DEVICE_LIST_STRATEGY=envvar`. With envvar mode, the device plugin does not need to load CUDA libraries itself.

---

### 2. CDI Spec Wrong hostPath

**Symptom**: Pod has `nvidia.com/gpu: 1` allocated but container cannot find NVIDIA libraries. `nvidia-smi` fails with `Failed to initialize NVML: driver/library version mismatch` or `No devices found`.

**Root cause**: CDI spec generated by device plugin contained `hostPath: /usr/lib/libEGL_nvidia.so.x`. That path does not exist on Talos.

**Fix**: `DEVICE_LIST_STRATEGY=envvar` — completely avoids CDI. The Talos container runtime handles library injection with Talos-aware paths.

---

### 3. Device Plugin Error: Invalid Strategy "envvars"

**Symptom**: Device plugin logs show `invalid strategy: envvars`.

**Root cause**: Typo in the env var value — `envvars` (plural) is not a valid strategy name.

**Fix**: Correct spelling is `envvar` (singular, no trailing s).

**Code change** (commit `5ee9915`):
```go
// Wrong:
{"name": "DEVICE_LIST_STRATEGY", "value": "envvars"},
// Correct:
{"name": "DEVICE_LIST_STRATEGY", "value": "envvar"},
```

---

### 4. Ollama Pod Pending: Stale Node Selector

**Symptom**: Ollama pod stuck in `Pending` with no node matching the selector.

**Root cause**: The cluster had an old label `node-role.kubernetes.io/gpu=true` on k8s-worker4 from a previous configuration. After the GPU operator reconfigured NFD, the correct label was `nvidia.com/gpu.present=true`. The Ollama Helm values were still using the old label.

**Fix**: Updated all `nodeSelector` in `ollama.go` and `nvidia_gpu_operator.go` to use `nvidia.com/gpu.present=true` — the label set automatically by NFD when it detects a GPU.

---

### 5. GPU Operator Reverts nvidia.com/gpu.deploy.driver Label

**Symptom**: Manually setting `nvidia.com/gpu.deploy.driver=true` on the node had no effect — GPU operator reset it to `pre-installed` within seconds.

**Root cause**: The GPU operator controller loop continuously reconciles this label based on what it detects on the node (Talos extension labels). It cannot be overridden via manual `kubectl label`.

**Resolution**: Do not fight it. The `DESIRED=0` driver DaemonSet is the correct behavior when Talos extensions are present. The GPU still works — the Talos extension driver is loaded and active.

---

### 6. PodSecurity Blocking GPU Operator Pods

**Symptom**: GPU operator pods and NFD worker DaemonSet failed with:
```
violates PodSecurity "baseline:latest": hostPath volumes (volume "host-os-release")
```

**Root cause**: The `nvidia-gpu-operator` namespace was created with the default `baseline` PodSecurity profile. GPU operator components require `hostPath` volumes and `privileged` containers.

**Fix**: Namespace created by CDK8s with the `privileged` PodSecurity label:
```go
Labels: &map[string]*string{
    "pod-security.kubernetes.io/enforce": jsii.String("privileged"),
},
```

---

## Verification Commands

```bash
# Check the GPU node has the NFD label applied
kubectl get node k8s-worker4 --show-labels | grep nvidia

# Expected output includes:
# nvidia.com/gpu.present=true
# nvidia.com/gpu.memory=16384Mi
# nvidia.com/gpu.product=NVIDIA-GeForce-RTX-5070-Ti

# Check device plugin is running (not Init:0/1)
kubectl get pods -n nvidia-gpu-operator | grep device-plugin

# Expected:
# nvidia-device-plugin-daemonset-xxxxx   1/1   Running   0   <age>

# Check the validation bridge is running
kubectl get pods -n nvidia-gpu-operator | grep validation-bridge

# Verify GPU resource is advertisable on the node
kubectl describe node k8s-worker4 | grep -A5 "Allocatable"

# Expected: nvidia.com/gpu: 1

# Check a running GPU pod can see the GPU
kubectl exec -n ollama deploy/ollama -- nvidia-smi

# Expected: table showing RTX 5070 Ti, driver 570.x, CUDA 12.8

# Check the RuntimeClass exists
kubectl get runtimeclass nvidia
```
